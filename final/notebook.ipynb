{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed69f7e2",
   "metadata": {},
   "source": [
    "# ML/Net Leaderboard Extension: Fast Packet-Sequence + Flow-Summary Features\n",
    "\n",
    "**Goal.** Reproduce a strong baseline for traffic classification from pcapng data and extend feature representations beyond the standard fixed-length packet sequence.\n",
    "\n",
    "**What I did.**\n",
    "- Parse a labeled pcapng into per-sample packet sequences.\n",
    "- Build two feature sets:\n",
    "  1) **Baseline**: first *P* packets, packet lengths + inter-arrival times + protocol indicators.\n",
    "  2) **Extended**: baseline + flow summary statistics (duration, byte stats, proto fractions, port-bucket histograms).\n",
    "- Evaluate models under:\n",
    "  - **Random stratified split** (standard ML setting)\n",
    "  - **Temporal stratified split** (more realistic; reduces leakage and captures drift)\n",
    "\n",
    "**Outputs.**\n",
    "- `results/leaderboard.csv` (main comparison table)\n",
    "- `results/ablation.csv` (P ablation across feature sets + split types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3acab",
   "metadata": {},
   "source": [
    "## 1. Setup and Reproducibility\n",
    "\n",
    "This notebook is designed to run end-to-end with one click (\"Restart Kernel and Run All\").\n",
    "\n",
    "**Dependencies.** Installs `python-pcapng` and `dpkt` if missing.\n",
    "\n",
    "**Data requirement.**\n",
    "Place the capture file at:\n",
    "- `data/traffic.pcapng`\n",
    "\n",
    "The notebook asserts the file exists and prints its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ccdcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"scikit-learn\",\n",
    "    \"matplotlib\",\n",
    "    \"tqdm\",\n",
    "    \"python-pcapng\",\n",
    "    \"pcapng\",\n",
    "    \"dpkt\",\n",
    "    \"gdown\",\n",
    "]\n",
    "\n",
    "failed = []\n",
    "for p in pkgs:\n",
    "    try:\n",
    "        print(f\"Installing/upgrading: {p}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", p])\n",
    "    except subprocess.CalledProcessError:\n",
    "        failed.append(p)\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFAILED installs:\", failed)\n",
    "    print(\"Scroll up to see pip's error message for the failing package(s).\")\n",
    "else:\n",
    "    print(\"\\nAll dependencies installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b537e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess, glob, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "CWD = Path.cwd()\n",
    "if (CWD / \"final\").is_dir():\n",
    "    REPO_ROOT = CWD\n",
    "elif CWD.name == \"final\" and (CWD / \"template.md\").exists():\n",
    "    REPO_ROOT = CWD.parent\n",
    "else:\n",
    "    REPO_ROOT = next((p for p in [CWD] + list(CWD.parents) if (p / \"final\").is_dir()), None)\n",
    "    if REPO_ROOT is None:\n",
    "        raise FileNotFoundError(\"Couldn't find repo root containing a 'final/' directory.\")\n",
    "\n",
    "FINAL_DIR = REPO_ROOT / \"final\"\n",
    "DATA_DIR  = FINAL_DIR / \"data\"\n",
    "RESULTS_DIR = FINAL_DIR / \"results\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DRIVE_URL = \"https://drive.google.com/file/d/1uEbrL9fl1kd5hDCziSjTEnzbtXUZYIdM/view\"\n",
    "GZ_PATH   = DATA_DIR / \"traffic_download.gz\"\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"gdown\", \"--fuzzy\", DRIVE_URL, \"-O\", str(GZ_PATH)])\n",
    "subprocess.check_call([\"bash\", \"-lc\", f'gunzip -fk \"{GZ_PATH}\"'])\n",
    "\n",
    "# newest non-.gz file after gunzip\n",
    "non_gz = sorted([p for p in DATA_DIR.iterdir() if p.is_file() and p.suffix != \".gz\"],\n",
    "                key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not non_gz:\n",
    "    raise FileNotFoundError(f\"After gunzip, no output file found in {DATA_DIR}\")\n",
    "out_file = non_gz[0]\n",
    "\n",
    "file_desc = subprocess.check_output([\"bash\", \"-lc\", f'file -b \"{out_file}\"'], text=True).strip().lower()\n",
    "if \"tar archive\" in file_desc:\n",
    "    subprocess.check_call([\"bash\", \"-lc\", f'tar -xf \"{out_file}\" -C \"{DATA_DIR}\"'])\n",
    "\n",
    "pcaps = glob.glob(str(DATA_DIR / \"**\" / \"*.pcapng\"), recursive=True) + \\\n",
    "        glob.glob(str(DATA_DIR / \"**\" / \"*.pcap\"), recursive=True)\n",
    "\n",
    "if pcaps:\n",
    "    pcap_path = pcaps[0]\n",
    "else:\n",
    "    pcap_path = str(DATA_DIR / \"traffic.pcapng\")\n",
    "    shutil.move(str(out_file), pcap_path)\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"PCAP ready at:\", pcap_path, \"size(MB)=\", round(os.path.getsize(pcap_path)/1024/1024, 2))\n",
    "\n",
    "CONFIG = {\n",
    "    \"pcap_path\": pcap_path,\n",
    "    \"label_level\": \"easy\",\n",
    "    \"N_target\": 30000,\n",
    "    \"test_frac\": 0.2,\n",
    "    \"P_default\": 20,\n",
    "    \"P_list\": [5, 10, 20, 40],\n",
    "    \"max_packets_per_sample_parse\": 200,\n",
    "}\n",
    "print(\"Config:\", CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46d488-2120-4ffe-b3c2-7e2b4b8e26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, time, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c04621-a55e-481b-aea7-c0cb2a77fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcapng.scanner import FileScanner\n",
    "import dpkt\n",
    "print(\"Imports OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab9103-fa51-480a-8d51-8401091ebd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcap_path = CONFIG[\"pcap_path\"]\n",
    "print(\"pcap_path:\", pcap_path)\n",
    "print(\"exists:\", os.path.exists(pcap_path))\n",
    "assert os.path.exists(pcap_path), f\"Missing {pcap_path}. Put traffic.pcapng in data/.\"\n",
    "print(\"size (MB):\", os.path.getsize(pcap_path) / (1024**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07cf76",
   "metadata": {},
   "source": [
    "## 2. Parse pcapng into labeled samples\n",
    "\n",
    "Each packet contains an `opt_comment` field with format:\n",
    "`sampleID,easy_medium_hard`.\n",
    "\n",
    "For each sample (flow), we store:\n",
    "- packet timestamps\n",
    "- signed packet lengths (direction inferred by first seen source)\n",
    "- L4 protocol and ports (TCP/UDP + port values)\n",
    "\n",
    "To keep runtime manageable, we cap packets per sample to `max_packets_per_sample_parse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa79ff9-4054-44e4-808a-f09297930357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def parse_comment(comment: str):\n",
    "    if not comment:\n",
    "        return None\n",
    "    parts = comment.strip().split(\",\")\n",
    "    if len(parts) < 2:\n",
    "        return None\n",
    "    sid = parts[0].strip()\n",
    "    labels = parts[1].strip().split(\"_\")\n",
    "    easy = labels[0].strip() if len(labels) >= 1 else None\n",
    "    med  = labels[1].strip() if len(labels) >= 2 else None\n",
    "    hard = labels[2].strip() if len(labels) >= 3 else None\n",
    "    return sid, easy, med, hard\n",
    "\n",
    "def parse_ip_l4(pkt_bytes: bytes):\n",
    "    ip = None\n",
    "    try:\n",
    "        eth = dpkt.ethernet.Ethernet(pkt_bytes)\n",
    "        ip = eth.data\n",
    "    except Exception:\n",
    "        ip = None\n",
    "\n",
    "    if ip is None:\n",
    "        try:\n",
    "            ip = dpkt.ip.IP(pkt_bytes)\n",
    "        except Exception:\n",
    "            try:\n",
    "                ip = dpkt.ip6.IP6(pkt_bytes)\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "    if isinstance(ip, dpkt.ip.IP):\n",
    "        src_id = \".\".join(map(str, ip.src))\n",
    "        dst_id = \".\".join(map(str, ip.dst))\n",
    "        proto = ip.p\n",
    "        l4 = ip.data\n",
    "    elif isinstance(ip, dpkt.ip6.IP6):\n",
    "        src_id = ip.src.hex()\n",
    "        dst_id = ip.dst.hex()\n",
    "        proto = ip.nxt\n",
    "        l4 = ip.data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    sport = dport = 0\n",
    "    if isinstance(l4, dpkt.tcp.TCP):\n",
    "        sport, dport = int(l4.sport), int(l4.dport)\n",
    "    elif isinstance(l4, dpkt.udp.UDP):\n",
    "        sport, dport = int(l4.sport), int(l4.dport)\n",
    "\n",
    "    return src_id, dst_id, int(proto), sport, dport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb192f9e-5194-4ed7-94a5-1e797ff49a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "samples: Dict[str, Dict[str, Any]] = {}\n",
    "total_packets = 0\n",
    "skipped_packets = 0\n",
    "cap = CONFIG[\"max_packets_per_sample_parse\"]\n",
    "\n",
    "with open(pcap_path, \"rb\") as f:\n",
    "    scanner = FileScanner(f)\n",
    "    for block in tqdm(scanner, desc=\"Parsing pcapng (EnhancedPacket)\"):\n",
    "        if block.__class__.__name__ != \"EnhancedPacket\":\n",
    "            continue\n",
    "\n",
    "        total_packets += 1\n",
    "        raw = block.packet_data\n",
    "        ts = float(block.timestamp)\n",
    "\n",
    "        opts = getattr(block, \"options\", None)\n",
    "        comment = opts.get(\"opt_comment\") if opts else None\n",
    "        parsed = parse_comment(comment) if comment else None\n",
    "        if parsed is None:\n",
    "            skipped_packets += 1\n",
    "            continue\n",
    "\n",
    "        sid, easy, med, hard = parsed\n",
    "        label = easy if CONFIG[\"label_level\"] == \"easy\" else (med if CONFIG[\"label_level\"] == \"medium\" else hard)\n",
    "\n",
    "        info = parse_ip_l4(raw)\n",
    "        if info is None:\n",
    "            skipped_packets += 1\n",
    "            continue\n",
    "\n",
    "        src, dst, proto, sport, dport = info\n",
    "        plen = len(raw)\n",
    "\n",
    "        if sid not in samples:\n",
    "            samples[sid] = {\n",
    "                \"label\": label,\n",
    "                \"first_src\": src,\n",
    "                \"times\": [],\n",
    "                \"lens\": [],\n",
    "                \"protos\": [],\n",
    "                \"sports\": [],\n",
    "                \"dports\": [],\n",
    "            }\n",
    "\n",
    "        if len(samples[sid][\"times\"]) >= cap:\n",
    "            continue\n",
    "\n",
    "        sign = 1.0 if src == samples[sid][\"first_src\"] else -1.0\n",
    "        samples[sid][\"times\"].append(ts)\n",
    "        samples[sid][\"lens\"].append(sign * plen)\n",
    "        samples[sid][\"protos\"].append(proto)\n",
    "        samples[sid][\"sports\"].append(sport)\n",
    "        samples[sid][\"dports\"].append(dport)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(\"packets_total:\", total_packets)\n",
    "print(\"samples_found:\", len(samples))\n",
    "print(\"skipped_packets:\", skipped_packets)\n",
    "print(f\"parse_time_sec: {elapsed:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866031be",
   "metadata": {},
   "source": [
    "## 3. Dataset summary and stratified downsampling\n",
    "\n",
    "We construct a per-sample metadata table (`meta`) and then downsample to `N_target` samples\n",
    "**stratified by class label**, so class balance is preserved while improving runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19699f-73e8-48ce-9817-b01900a6efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = pd.Series([v[\"label\"] for v in samples.values()])\n",
    "display(lab.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b14644-96fa-46a5-b7e7-6ad283026952",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for sid, s in samples.items():\n",
    "    if not s[\"times\"]:\n",
    "        continue\n",
    "    rows.append({\n",
    "        \"sid\": sid,\n",
    "        \"label\": s[\"label\"],\n",
    "        \"t_first\": float(np.min(s[\"times\"])),\n",
    "        \"n_pkts\": len(s[\"times\"]),\n",
    "    })\n",
    "meta = pd.DataFrame(rows)\n",
    "print(\"meta shape:\", meta.shape)\n",
    "display(meta[\"label\"].value_counts())\n",
    "\n",
    "N_target = CONFIG[\"N_target\"]\n",
    "\n",
    "parts = []\n",
    "for lbl, sub in meta.groupby(\"label\"):\n",
    "    frac = len(sub) / len(meta)\n",
    "    k = int(round(N_target * frac))\n",
    "    k = max(50, k)\n",
    "    k = min(k, len(sub))\n",
    "    parts.append(sub.sample(n=k, random_state=SEED))\n",
    "\n",
    "meta_s = pd.concat(parts, ignore_index=True).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "print(\"sampled meta shape:\", meta_s.shape)\n",
    "display(meta_s[\"label\"].value_counts())\n",
    "\n",
    "sids = meta_s[\"sid\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2746da8",
   "metadata": {},
   "source": [
    "## 4. Feature representations\n",
    "\n",
    "We featurize each sample using the first **P packets**.\n",
    "\n",
    "### 4.1 Baseline (sequence only)\n",
    "For each of the first P packets, we create:\n",
    "- signed packet length\n",
    "- log(1 + inter-arrival time)\n",
    "- protocol indicators: TCP / UDP / other\n",
    "\n",
    "This yields **5P** features.\n",
    "\n",
    "### 4.2 Extended (sequence + summary)\n",
    "Adds 16 summary features:\n",
    "- log(1 + duration)\n",
    "- total bytes, mean length, std length, fraction upstream\n",
    "- protocol fractions (TCP/UDP/other)\n",
    "- source/destination port bucket histograms (none / <1024 / 1024–49151 / 49152+)\n",
    "\n",
    "Total dimension: **5P + 16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed819f1-a229-4e4c-a5b3-6614b9827b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_bucket(p: int) -> int:\n",
    "    if p <= 0:\n",
    "        return 0\n",
    "    if p < 1024:\n",
    "        return 1\n",
    "    if p < 49152:\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "def build_features_for_sample(times, lens_signed, protos, sports, dports, P: int, extended: bool):\n",
    "    times = np.asarray(times, dtype=float)\n",
    "    lens_signed = np.asarray(lens_signed, dtype=float)\n",
    "    protos = np.asarray(protos, dtype=int)\n",
    "    sports = np.asarray(sports, dtype=int)\n",
    "    dports = np.asarray(dports, dtype=int)\n",
    "\n",
    "    # sort within sample\n",
    "    idx = np.argsort(times)\n",
    "    times, lens_signed, protos, sports, dports = times[idx], lens_signed[idx], protos[idx], sports[idx], dports[idx]\n",
    "\n",
    "    # iat + stabilize scale\n",
    "    iat = np.diff(times, prepend=times[0])\n",
    "    iat = np.log1p(np.clip(iat, 0.0, None))\n",
    "\n",
    "    def pad(arr, fill=0.0):\n",
    "        arr = arr[:P]\n",
    "        if len(arr) < P:\n",
    "            arr = np.pad(arr, (0, P-len(arr)), constant_values=fill)\n",
    "        return arr\n",
    "\n",
    "    L = pad(lens_signed, 0.0)\n",
    "    T = pad(iat, 0.0)\n",
    "    tcp   = pad((protos == 6).astype(float), 0.0)\n",
    "    udp   = pad((protos == 17).astype(float), 0.0)\n",
    "    other = pad(((protos != 6) & (protos != 17)).astype(float), 0.0)\n",
    "\n",
    "    seq = np.stack([L, T, tcp, udp, other], axis=1).reshape(-1)\n",
    "\n",
    "    if not extended:\n",
    "        return seq\n",
    "\n",
    "    # summary features\n",
    "    duration = float(np.log1p(max(times[-1] - times[0], 0.0))) if len(times) > 1 else 0.0\n",
    "    abs_lens = np.abs(lens_signed)\n",
    "    total_bytes = float(abs_lens.sum())\n",
    "    mean_len = float(abs_lens.mean()) if len(abs_lens) else 0.0\n",
    "    std_len  = float(abs_lens.std()) if len(abs_lens) else 0.0\n",
    "    frac_up  = float((lens_signed > 0).mean()) if len(lens_signed) else 0.0\n",
    "\n",
    "    frac_tcp = float((protos == 6).mean()) if len(protos) else 0.0\n",
    "    frac_udp = float((protos == 17).mean()) if len(protos) else 0.0\n",
    "    frac_oth = 1.0 - frac_tcp - frac_udp\n",
    "\n",
    "    sb = np.array([port_bucket(p) for p in sports], dtype=int)\n",
    "    db = np.array([port_bucket(p) for p in dports], dtype=int)\n",
    "    sb_hist = np.bincount(sb, minlength=4).astype(float)\n",
    "    db_hist = np.bincount(db, minlength=4).astype(float)\n",
    "    denom = float(len(times)) if len(times) else 1.0\n",
    "    sb_hist /= denom\n",
    "    db_hist /= denom\n",
    "\n",
    "    summary = np.array([\n",
    "        duration, total_bytes, mean_len, std_len, frac_up,\n",
    "        frac_tcp, frac_udp, frac_oth,\n",
    "        *sb_hist.tolist(), *db_hist.tolist()\n",
    "    ], dtype=float)\n",
    "\n",
    "    return np.concatenate([seq, summary], axis=0)\n",
    "\n",
    "def build_X_y_ts_from_samples(samples: dict, sids: list, P: int, extended: bool):\n",
    "    X, y, ts = [], [], []\n",
    "    for sid in sids:\n",
    "        s = samples[sid]\n",
    "        if not s[\"times\"]:\n",
    "            continue\n",
    "        feat = build_features_for_sample(\n",
    "            s[\"times\"], s[\"lens\"], s[\"protos\"], s[\"sports\"], s[\"dports\"],\n",
    "            P=P, extended=extended\n",
    "        )\n",
    "        X.append(feat)\n",
    "        y.append(s[\"label\"])\n",
    "        ts.append(float(np.min(s[\"times\"])))\n",
    "    return np.vstack(X), np.array(y), np.array(ts, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf45683",
   "metadata": {},
   "source": [
    "## 5. Evaluation splits\n",
    "\n",
    "We compare two evaluation settings:\n",
    "\n",
    "### 5.1 Random stratified split\n",
    "Standard i.i.d. ML assumption: train/test are randomly mixed but class-balanced.\n",
    "\n",
    "### 5.2 Temporal stratified split (leakage-aware)\n",
    "For each class, sort samples by time and assign the most recent fraction to test.\n",
    "This preserves temporal ordering while preventing label drop.\n",
    "This split is a proxy for distribution shift / drift over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db9282-3c57-4953-a568-c56165db51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, y_str_sanity, ts_sanity = build_X_y_ts_from_samples(\n",
    "    samples, meta_s.head(100)[\"sid\"].tolist(), P=CONFIG[\"P_default\"], extended=False\n",
    ")\n",
    "Xe, _, _ = build_X_y_ts_from_samples(\n",
    "    samples, meta_s.head(100)[\"sid\"].tolist(), P=CONFIG[\"P_default\"], extended=True\n",
    ")\n",
    "print(\"baseline feat dim:\", Xb.shape[1])\n",
    "print(\"extended feat dim:\", Xe.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e8a57",
   "metadata": {},
   "source": [
    "## 6. Models and metrics\n",
    "\n",
    "We evaluate three classic classifiers:\n",
    "- Logistic Regression (class-weighted, standardized)\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "**Metrics**\n",
    "- Balanced Accuracy (primary): robust to class imbalance\n",
    "- Macro F1 (secondary)\n",
    "\n",
    "We also plot a confusion matrix for the best model in the temporal+extended setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e0292-d7fa-4e03-bb72-022d3d7d5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "def stratified_temporal_split_idx(y_enc, t_first, test_frac=0.2):\n",
    "    y_enc = np.asarray(y_enc)\n",
    "    t_first = np.asarray(t_first)\n",
    "\n",
    "    tr, te = [], []\n",
    "    for c in np.unique(y_enc):\n",
    "        c_idx = np.where(y_enc == c)[0]\n",
    "        c_idx = c_idx[np.argsort(t_first[c_idx])]\n",
    "        k = max(1, int(len(c_idx) * test_frac))\n",
    "        te.extend(c_idx[-k:])\n",
    "        tr.extend(c_idx[:-k])\n",
    "\n",
    "    return np.array(tr, dtype=int), np.array(te, dtype=int)\n",
    "\n",
    "models = {\n",
    "    \"LogReg_bal\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=3000, class_weight=\"balanced\"))\n",
    "    ]),\n",
    "    \"RF\": RandomForestClassifier(n_estimators=400, random_state=SEED, n_jobs=-1),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=800, random_state=SEED, n_jobs=-1),\n",
    "}\n",
    "\n",
    "def eval_table(X, y_enc, tr_idx, te_idx, setting: str):\n",
    "    rows = []\n",
    "    for name, model in models.items():\n",
    "        model.fit(X[tr_idx], y_enc[tr_idx])\n",
    "        pred = model.predict(X[te_idx])\n",
    "        rows.append({\n",
    "            \"setting\": setting,\n",
    "            \"model\": name,\n",
    "            \"balanced_accuracy\": float(balanced_accuracy_score(y_enc[te_idx], pred)),\n",
    "            \"macro_f1\": float(f1_score(y_enc[te_idx], pred, average=\"macro\")),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"balanced_accuracy\", ascending=False)\n",
    "\n",
    "def plot_confusion(y_true, y_pred, class_names, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks, class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6ab72",
   "metadata": {},
   "source": [
    "## 7. Main results\n",
    "\n",
    "We compare:\n",
    "- baseline vs extended features\n",
    "- random vs temporal stratified splits\n",
    "\n",
    "We report balanced accuracy and macro-F1 for each model and setting, then select the best\n",
    "**temporal stratified + extended** model and visualize its confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ba49e-7996-46dc-a093-4e7c1230ba6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P = CONFIG[\"P_default\"]\n",
    "\n",
    "X_base, y_str, ts = build_X_y_ts_from_samples(samples, sids, P=P, extended=False)\n",
    "X_ext,  _,    _   = build_X_y_ts_from_samples(samples, sids, P=P, extended=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y_str)\n",
    "\n",
    "idx = np.arange(len(y_enc))\n",
    "tr_r, te_r = train_test_split(\n",
    "    idx,\n",
    "    test_size=CONFIG[\"test_frac\"],\n",
    "    random_state=SEED,\n",
    "    stratify=y_enc\n",
    ")\n",
    "\n",
    "tr_t, te_t = stratified_temporal_split_idx(y_enc, ts, test_frac=CONFIG[\"test_frac\"])\n",
    "\n",
    "tbl = pd.concat([\n",
    "    eval_table(X_base, y_enc, tr_r, te_r, f\"baseline P={P} / random\"),\n",
    "    eval_table(X_base, y_enc, tr_t, te_t, f\"baseline P={P} / temporal_strat\"),\n",
    "    eval_table(X_ext,  y_enc, tr_r, te_r, f\"extended P={P} / random\"),\n",
    "    eval_table(X_ext,  y_enc, tr_t, te_t, f\"extended P={P} / temporal_strat\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "display(tbl)\n",
    "\n",
    "# Confusion matrix for best temporal+extended\n",
    "best_row = (\n",
    "    tbl[tbl[\"setting\"].str.contains(\"extended\") & tbl[\"setting\"].str.contains(\"temporal_strat\")]\n",
    "    .sort_values(\"balanced_accuracy\", ascending=False)\n",
    "    .iloc[0]\n",
    ")\n",
    "best_model_name = best_row[\"model\"]\n",
    "print(\"Best temporal+extended:\", best_row.to_dict())\n",
    "\n",
    "model = models[best_model_name]\n",
    "model.fit(X_ext[tr_t], y_enc[tr_t])\n",
    "pred = model.predict(X_ext[te_t])\n",
    "\n",
    "plot_confusion(\n",
    "    y_true=y_enc[te_t],\n",
    "    y_pred=pred,\n",
    "    class_names=list(le.classes_),\n",
    "    title=f\"Confusion ({best_model_name}) - extended P={P} temporal_strat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b46df8",
   "metadata": {},
   "source": [
    "### Interpretation (high level)\n",
    "- Extended features consistently improve performance over baseline.\n",
    "- Temporal stratified splits are harder (as expected), indicating time drift and reduced leakage.\n",
    "- Performance saturates quickly with P: most discriminative signal is present in early packets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507de76",
   "metadata": {},
   "source": [
    "## 8. Ablation: how many packets per sample?\n",
    "\n",
    "We vary P ∈ {5, 10, 20, 40} and compute best balanced accuracy across models for each:\n",
    "- baseline vs extended\n",
    "- random vs temporal stratified split\n",
    "\n",
    "This answers: **How many packets are needed before performance saturates?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa9172-a1f4-49bb-a6e3-2359b686817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "P_list = CONFIG[\"P_list\"]\n",
    "\n",
    "for P in P_list:\n",
    "    Xb, y_str, ts = build_X_y_ts_from_samples(samples, sids, P=P, extended=False)\n",
    "    Xe, _, _      = build_X_y_ts_from_samples(samples, sids, P=P, extended=True)\n",
    "\n",
    "    leP = LabelEncoder()\n",
    "    y_enc = leP.fit_transform(y_str)\n",
    "\n",
    "    idx = np.arange(len(y_enc))\n",
    "    tr_r, te_r = train_test_split(\n",
    "        idx,\n",
    "        test_size=CONFIG[\"test_frac\"],\n",
    "        random_state=SEED,\n",
    "        stratify=y_enc\n",
    "    )\n",
    "    tr_t, te_t = stratified_temporal_split_idx(y_enc, ts, test_frac=CONFIG[\"test_frac\"])\n",
    "\n",
    "    for feat_name, X in [(\"baseline\", Xb), (\"extended\", Xe)]:\n",
    "        for split_name, (tr, te) in [(\"random\", (tr_r, te_r)), (\"temporal_strat\", (tr_t, te_t))]:\n",
    "            for mname, model in models.items():\n",
    "                model.fit(X[tr], y_enc[tr])\n",
    "                pred = model.predict(X[te])\n",
    "                records.append({\n",
    "                    \"P\": P, \"features\": feat_name, \"split\": split_name, \"model\": mname,\n",
    "                    \"balanced_accuracy\": float(balanced_accuracy_score(y_enc[te], pred)),\n",
    "                    \"macro_f1\": float(f1_score(y_enc[te], pred, average=\"macro\")),\n",
    "                })\n",
    "\n",
    "df_ab = pd.DataFrame(records)\n",
    "display(\n",
    "    df_ab.sort_values([\"split\",\"features\",\"P\",\"balanced_accuracy\"], ascending=[True,True,True,False]).head(30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ede467-87a0-41ac-81c8-a8a1d10b4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = df_ab.groupby([\"P\",\"features\",\"split\"])[\"balanced_accuracy\"].max().reset_index()\n",
    "\n",
    "for split in [\"random\", \"temporal_strat\"]:\n",
    "    plt.figure()\n",
    "    for feat in [\"baseline\", \"extended\"]:\n",
    "        sub = best[(best[\"split\"] == split) & (best[\"features\"] == feat)]\n",
    "        plt.plot(sub[\"P\"], sub[\"balanced_accuracy\"], marker=\"o\", label=feat)\n",
    "    plt.xscale(\"log\", base=2)\n",
    "    plt.xlabel(\"Packets per sample (P)\")\n",
    "    plt.ylabel(\"Best Balanced Accuracy\")\n",
    "    plt.title(f\"Ablation: best balanced accuracy vs P ({split})\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923b60e",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- Best performance occurs at modest P (typically 10–20).\n",
    "- Larger P can degrade performance due to added noise / higher dimensionality.\n",
    "- Under temporal evaluation, smaller P can generalize better (less overfit to time-specific behavior)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b1a35",
   "metadata": {},
   "source": [
    "## 9. Saving artifacts\n",
    "\n",
    "We save:\n",
    "- `results/leaderboard.csv`: main comparison table\n",
    "- `results/ablation.csv`: full ablation record\n",
    "These files can be used directly in the Sphinx report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769ef46-da36-4190-b673-4d3a0518fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tbl.to_csv(RESULTS_DIR / \"leaderboard.csv\", index=False)\n",
    "df_ab.to_csv(RESULTS_DIR / \"ablation.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", RESULTS_DIR / \"leaderboard.csv\")\n",
    "print(\" -\", RESULTS_DIR / \"ablation.csv\")\n",
    "\n",
    "import platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Platform:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995df04",
   "metadata": {},
   "source": [
    "## Limitations and next steps\n",
    "\n",
    "- Direction is inferred using the first seen source address per sample; this is a heuristic.\n",
    "- Temporal split is per-class; it preserves labels but is not a full streaming evaluation.\n",
    "- Future improvements:\n",
    "  - try calibration + confidence\n",
    "  - measure compute cost vs accuracy (feature + training time)\n",
    "  - evaluate transfer: train on one capture time period, test on another capture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
